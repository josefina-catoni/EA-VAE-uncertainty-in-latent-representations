{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "## Importing libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from auxiliary_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/'.join(os.getcwd().split('/')[:-1])\n",
    "methods_dir = f'{ROOT_PATH}/Methods/MNIST_domains/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(methods_dir)\n",
    "from methods import VariationalAutoencoder,z_VariationalAutoencoder, set_seed, replace_point_by_underscore,CsikorDataset, MNISTShuffDataset\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/jcatoni/.medmnist/chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Van Hateren Dataset\n",
    "natural40_dir = f'{ROOT_PATH}/Datasets/VanHateren/'\n",
    "\n",
    "nat_test_labels = pkl.load(open(natural40_dir+'test_labels.pkl','rb'))\n",
    "nat_test_images = torch.tensor(np.load(natural40_dir+'test_images/test_images.npy').astype(np.float32).reshape(64000,1,40,40))\n",
    "nat_n_test = len(nat_test_labels)\n",
    "\n",
    "nat_train_pixs_mean = np.load(natural40_dir+'nat_train_pixs_mean.npy')\n",
    "nat_train_pixs_std = np.load(natural40_dir+'nat_train_pixs_std.npy')\n",
    " \n",
    "def rescale_nat2num(img):\n",
    "    old_mean = nat_train_pixs_mean\n",
    "    old_std = nat_train_pixs_std\n",
    "    \n",
    "    img = (1/(6*old_std)*(img - old_mean) + 0.5).clamp(0,1)\n",
    "    return img\n",
    "\n",
    "data_transform_natural_rescaled = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Lambda(lambda x: rescale_nat2num(x)) \n",
    "]) \n",
    "batch_size = 128\n",
    "nat_test_rescaled_dataset = CsikorDataset(nat_test_labels,nat_test_images,transform = data_transform_natural_rescaled)\n",
    "nat_test_rescaled_dataloader = DataLoader(nat_test_rescaled_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## MNIST Dataset\n",
    "data_dir = f'{ROOT_PATH}/Datasets/Mnist/'\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "transforms.Resize((32, 32)),\n",
    "transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "dataset = MNIST(root=data_dir+'MNIST', download=True, train=True, transform=img_transform)\n",
    "num_train_idxs = np.load(data_dir+'train_idxs.npy')\n",
    "num_val_idxs = np.load(data_dir+'val_idxs.npy')\n",
    "num_train_dataset = Subset(dataset,num_train_idxs)\n",
    "num_val_dataset = Subset(dataset,num_val_idxs)\n",
    "num_test_dataset = MNIST(root=data_dir+'MNIST', download=True, train=False, transform=img_transform)\n",
    "\n",
    "num_n_val = len(num_val_dataset)\n",
    "num_n_train = len(dataset)-num_n_val\n",
    "\n",
    "num_train_dataloader = DataLoader(num_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "num_validation_dataloader = DataLoader(num_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "num_test_dataloader = DataLoader(num_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## FashionMNIST Dataset\n",
    "data_dir = f'{ROOT_PATH}/Datasets/FashionMnist/'\n",
    "\n",
    "fash_test_dataset = FashionMNIST(root=data_dir+'FashionMNIST', download=True, train=False, transform=img_transform)\n",
    "fash_test_dataloader = DataLoader(fash_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#ChestMNIST Dataset\n",
    "data_flag = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "med_test_dataset = DataClass(split='test', transform=img_transform, download=download)\n",
    "med_n_test = len(med_test_dataset)\n",
    "med_test_dataloader = data.DataLoader(dataset=med_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "rnd_pix_shuff_imgs = torch.zeros(10000,1,32,32)\n",
    "# shuffle pixels across first axis\n",
    "for k in range(10000):\n",
    "    rnd_pix_shuff_imgs[k] = num_test_dataset[k][0]\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        torch.manual_seed(32*i+j)\n",
    "        rnd_pix_shuff_imgs[:,0,i,j] = rnd_pix_shuff_imgs[:,0,i,j][torch.randperm(10000)]\n",
    "        \n",
    "batch_size = 128\n",
    "\n",
    "num_test_ShuffPix_dataset = MNISTShuffDataset(num_test_dataset.targets,rnd_pix_shuff_imgs)\n",
    "num_test_ShuffPix_dataloader = DataLoader(num_test_ShuffPix_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_manuscript_training = True\n",
    "\n",
    "input_dim = 32*32\n",
    "capacity = 64\n",
    "\n",
    "if use_manuscript_training:\n",
    "    latent_dim_vae = 5\n",
    "    latent_dim_eavae = 4\n",
    "    variational_beta_y_vae = 4\n",
    "    variational_beta_y_eavae = 4\n",
    "    variational_beta_z_eavae = 1\n",
    "    learning_rate = 1e-3\n",
    "    in_location_VAE = f'{ROOT_PATH}/MNIST/Model_checkpoints/manuscript/VAE/'\n",
    "    in_location_EAVAE = f'{ROOT_PATH}/MNIST/Model_checkpoints/manuscript/EA-VAE/'\n",
    "else:\n",
    "    latent_dim_vae = 5 #complete\n",
    "    latent_dim_eavae = 4 #complete\n",
    "    variational_beta_y_vae = 4 #complete\n",
    "    variational_beta_y_eavae = 4 #complete\n",
    "    variational_beta_z_eavae = 1 #complete\n",
    "    learning_rate = 1e-3 #complete\n",
    "    in_location_VAE = f'{ROOT_PATH}/MNIST/Model_checkpoints/personal/latent_dim_{latent_dim_vae}/VAE/beta_y_'+replace_point_by_underscore(str(variational_beta_y_vae))+'/lr_'+replace_point_by_underscore(str(learning_rate))+'/'\n",
    "    in_location_EAVAE = f'{ROOT_PATH}/MNIST/Model_checkpoints/personal/latent_dim_{latent_dim_eavae}/EA-VAE/beta_y_'+replace_point_by_underscore(str(variational_beta_y_eavae))+'/beta_z_'+replace_point_by_underscore(str(variational_beta_z_eavae))+'/lr_'+replace_point_by_underscore(str(learning_rate))+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19497/962170611.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_vae = torch.load(in_location_VAE+'bestnet.pth')\n",
      "/tmp/ipykernel_19497/962170611.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_eavae = torch.load(in_location_EAVAE+'bestnet.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "z_VariationalAutoencoder(\n",
       "  (encoder): z_VEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (fc_mu): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    (fc_logvar): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    (fc_intz): Linear(in_features=16384, out_features=20, bias=True)\n",
       "    (fc_zmu): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (fc_zlogvar): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): z_VDecoder_BCE(\n",
       "    (fc): Linear(in_features=4, out_features=4096, bias=True)\n",
       "    (conv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_manuscript_training:\n",
    "   out_location = f'{ROOT_PATH}/Plot_results/MNIST/data/manuscript/prueba/'\n",
    "else:\n",
    "   out_location = f'{ROOT_PATH}/Plot_results/MNIST/data/personal/'\n",
    "\n",
    "if not os.path.exists(out_location):\n",
    "   os.makedirs(out_location)\n",
    "   print(\"Se creó el directorio: \", out_location)\n",
    "\n",
    "## loading models\n",
    "# checking if torch.cuda is available\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_vae = VariationalAutoencoder(num_n_train,num_n_val,input_dim,latent_dim_vae,capacity,learning_rate,rec_loss_method='BCE', device=device)\n",
    "model_eavae = z_VariationalAutoencoder(num_n_train,num_n_val,input_dim,latent_dim_eavae,capacity,learning_rate,rec_loss_method='BCE', device=device)\n",
    "\n",
    "state_dict_vae = torch.load(in_location_VAE+'bestnet.pth')\n",
    "state_dict_eavae = torch.load(in_location_EAVAE+'bestnet.pth')\n",
    "\n",
    "model_vae.load_state_dict(state_dict_vae)\n",
    "model_eavae.load_state_dict(state_dict_eavae)\n",
    "\n",
    "model_vae = model_vae.to(device)\n",
    "model_eavae = model_eavae.to(device)\n",
    "\n",
    "model_vae.eval()\n",
    "model_eavae.eval()\n",
    "\n",
    "#cost_vae = torch.load(in_location_VAE+'finalloss.pth')\n",
    "#cost_eavae = torch.load(in_location_EAVAE+'finalloss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_avimg = torch.zeros(1,32,32)\n",
    "for k, (x, y) in enumerate(num_test_dataloader):\n",
    "    num_avimg += x.sum(dim=0)\n",
    "num_avimg = num_avimg/len(num_test_dataloader.dataset)\n",
    "\n",
    "num_test_mu_vae, num_test_var_vae, num_test_zmu_vae, num_test_zvar_vae = data_posteriors(model_vae, num_test_dataloader, latent_dim_vae, False)\n",
    "num_test_mu_eavae, num_test_var_eavae, num_test_zmu_eavae, num_test_zvar_eavae = data_posteriors(model_eavae, num_test_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "num_test_av_post_mu_vae, num_test_av_post_var_vae = average_posterior(num_test_mu_vae, num_test_var_vae)\n",
    "num_test_av_post_mu_eavae, num_test_av_post_var_eavae = average_posterior(num_test_mu_eavae, num_test_var_eavae)\n",
    "\n",
    "num_uncertain_mu_vae, num_uncertain_var_vae, num_uncertain_zmu_vae, num_uncertain_zvar_vae = individual_posterior(model_vae, num_avimg, False)\n",
    "num_uncertain_mu_eavae, num_uncertain_var_eavae, num_uncertain_zmu_eavae, num_uncertain_zvar_eavae = individual_posterior(model_eavae, num_avimg, True)\n",
    "\n",
    "med_test_mu_vae, med_test_var_vae, med_test_zmu_vae, med_test_zvar_vae = data_posteriors(model_vae, med_test_dataloader, latent_dim_vae, False)\n",
    "med_test_mu_eavae, med_test_var_eavae, med_test_zmu_eavae, med_test_zvar_eavae = data_posteriors(model_eavae, med_test_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "nat_test_mu_vae, nat_test_var_vae, nat_test_zmu_vae, nat_test_zvar_vae = data_posteriors(model_vae, nat_test_rescaled_dataloader, latent_dim_vae, False)\n",
    "nat_test_mu_eavae, nat_test_var_eavae, nat_test_zmu_eavae, nat_test_zvar_eavae = data_posteriors(model_eavae, nat_test_rescaled_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "fash_test_mu_vae, fash_test_var_vae, fash_test_zmu_vae, fash_test_zvar_vae = data_posteriors(model_vae, fash_test_dataloader, latent_dim_vae, False)\n",
    "fash_test_mu_eavae, fash_test_var_eavae, fash_test_zmu_eavae, fash_test_zvar_eavae = data_posteriors(model_eavae, fash_test_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "num_test_ShuffPix_mu_vae, num_test_ShuffPix_var_vae, num_test_ShuffPix_zmu_vae, num_test_ShuffPix_zvar_vae = data_posteriors(model_vae, num_test_ShuffPix_dataloader, latent_dim_vae, False)\n",
    "num_test_ShuffPix_mu_eavae, num_test_ShuffPix_var_eavae, num_test_ShuffPix_zmu_eavae, num_test_ShuffPix_zvar_eavae = data_posteriors(model_eavae, num_test_ShuffPix_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "num_uncertainty_vae_df = pd.DataFrame({'uncertainty':np.sqrt(num_test_var_vae).mean(axis=1), 'model':'VAE', 'data-set':'MNIST'})\n",
    "med_uncertainty_vae_df = pd.DataFrame({'uncertainty':np.sqrt(med_test_var_vae).mean(axis=1), 'model':'VAE', 'data-set':'ChestMNIST'})\n",
    "nat_uncertainty_vae_df = pd.DataFrame({'uncertainty':np.sqrt(nat_test_var_vae).mean(axis=1), 'model':'VAE', 'data-set':'van Hateren'})\n",
    "fash_uncertainty_vae_df = pd.DataFrame({'uncertainty':np.sqrt(fash_test_var_vae).mean(axis=1), 'model':'VAE', 'data-set':'FashionMNIST'})\n",
    "num_ShuffPix_uncertainty_vae_df = pd.DataFrame({'uncertainty':np.sqrt(num_test_ShuffPix_var_vae).mean(axis=1), 'model':'VAE', 'data-set':'MNIST permuted pixels'})\n",
    "\n",
    "num_uncertainty_eavae_df = pd.DataFrame({'uncertainty':np.sqrt(num_test_var_eavae).mean(axis=1), 'model':'EA-VAE', 'data-set':'MNIST'})\n",
    "med_uncertainty_eavae_df = pd.DataFrame({'uncertainty':np.sqrt(med_test_var_eavae).mean(axis=1), 'model':'EA-VAE', 'data-set':'ChestMNIST'})\n",
    "nat_uncertainty_eavae_df = pd.DataFrame({'uncertainty':np.sqrt(nat_test_var_eavae).mean(axis=1), 'model':'EA-VAE', 'data-set':'van Hateren'})\n",
    "fash_uncertainty_eavae_df = pd.DataFrame({'uncertainty':np.sqrt(fash_test_var_eavae).mean(axis=1), 'model':'EA-VAE', 'data-set':'FashionMNIST'})\n",
    "num_ShuffPix_uncertainty_eavae_df = pd.DataFrame({'uncertainty':np.sqrt(num_test_ShuffPix_var_eavae).mean(axis=1), 'model':'EA-VAE', 'data-set':'MNIST permuted pixels'})\n",
    "\n",
    "fash_zmu_vae_df = pd.DataFrame({'z_mu':fash_test_zmu_vae, 'model':'VAE', 'data-set':'FashionMNIST'})\n",
    "med_zmu_vae_df = pd.DataFrame({'z_mu':med_test_zmu_vae, 'model':'VAE', 'data-set':'ChestMNIST'})\n",
    "num_zmu_vae_df = pd.DataFrame({'z_mu':num_test_zmu_vae, 'model':'VAE', 'data-set':'MNIST'})\n",
    "nat_zmu_vae_df = pd.DataFrame({'z_mu':nat_test_zmu_vae, 'model':'VAE', 'data-set':'van Hateren'})\n",
    "num_ShuffPix_zmu_vae_df = pd.DataFrame({'z_mu':num_test_ShuffPix_zmu_vae, 'model':'VAE', 'data-set':'MNIST permuted pixels'})\n",
    "\n",
    "fash_zmu_eavae_df = pd.DataFrame({'z_mu':np.exp(fash_test_zmu_eavae + fash_test_zvar_eavae/2), 'model':'EA-VAE', 'data-set':'FashionMNIST'})\n",
    "med_zmu_eavae_df = pd.DataFrame({'z_mu':np.exp(med_test_zmu_eavae + med_test_zvar_eavae/2), 'model':'EA-VAE', 'data-set':'ChestMNIST'})\n",
    "num_zmu_eavae_df = pd.DataFrame({'z_mu':np.exp(num_test_zmu_eavae + num_test_zvar_eavae/2), 'model':'EA-VAE', 'data-set':'MNIST'})\n",
    "nat_zmu_eavae_df = pd.DataFrame({'z_mu':np.exp(nat_test_zmu_eavae + nat_test_zvar_eavae/2), 'model':'EA-VAE', 'data-set':'van Hateren'})\n",
    "num_ShuffPix_zmu_eavae_df = pd.DataFrame({'z_mu':np.exp(num_test_ShuffPix_zmu_eavae+num_test_ShuffPix_zvar_eavae/2), 'model':'EA-VAE', 'data-set':'MNIST permuted pixels'})\n",
    "\n",
    "uncertainty_df = pd.concat([num_uncertainty_vae_df,   num_ShuffPix_uncertainty_vae_df,   fash_uncertainty_vae_df,   med_uncertainty_vae_df,   nat_uncertainty_vae_df,\n",
    "                            num_uncertainty_eavae_df, num_ShuffPix_uncertainty_eavae_df, fash_uncertainty_eavae_df, med_uncertainty_eavae_df, nat_uncertainty_eavae_df])\n",
    "\n",
    "zmu_eavae_df = pd.concat([num_zmu_eavae_df,num_ShuffPix_zmu_eavae_df, fash_zmu_eavae_df, med_zmu_eavae_df, nat_zmu_eavae_df])\n",
    "\n",
    "uncertainty_vae_eavae_df = pd.concat([num_uncertainty_vae_df, num_uncertainty_eavae_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae.eval()\n",
    "model_eavae.eval()\n",
    "\n",
    "x_ori = []\n",
    "x_vae = []\n",
    "x_eavae = []\n",
    "for x, y in num_test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x_hat_eavae, _, _, _, _ = model_eavae(x.to(model_eavae.device), only_mu=True, only_zmu=True)\n",
    "        x_hat_vae, _, _ = model_vae(x.to(model_vae.device), only_mu=True)\n",
    "    for i,k in enumerate(range(3,6)):            \n",
    "        x_ori.append(x.detach().cpu().numpy()[k])\n",
    "        x_vae.append(x_hat_vae.detach().cpu().numpy()[k])\n",
    "        x_eavae.append(x_hat_eavae.detach().cpu().numpy()[k])\n",
    "    break # just one batch\n",
    "\n",
    "x_med_ori = []\n",
    "x_med_vae = []\n",
    "x_med_eavae = []\n",
    "for x, y in med_test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x_hat_eavae, _, _, _, _ = model_eavae(x.to(model_eavae.device), only_mu=True, only_zmu=True)\n",
    "        x_hat_vae, _, _ = model_vae(x.to(model_vae.device), only_mu=True)\n",
    "    for i,k in enumerate(range(3,6)):\n",
    "        x_med_ori.append(x.detach().cpu().numpy()[k])\n",
    "        x_med_vae.append(x_hat_vae.detach().cpu().numpy()[k])\n",
    "        x_med_eavae.append(x_hat_eavae.detach().cpu().numpy()[k])\n",
    "    break # just one batch\n",
    "\n",
    "x_fash_ori = []\n",
    "x_fash_vae = []\n",
    "x_fash_eavae = []\n",
    "for x, y in fash_test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x_hat_eavae, _, _, _, _ = model_eavae(x.to(model_eavae.device), only_mu=True, only_zmu=True)\n",
    "        x_hat_vae, _, _ = model_vae(x.to(model_vae.device), only_mu=True)\n",
    "    for i,k in enumerate(range(3,6)):\n",
    "        x_fash_ori.append(x.detach().cpu().numpy()[k])\n",
    "        x_fash_vae.append(x_hat_vae.detach().cpu().numpy()[k])\n",
    "        x_fash_eavae.append(x_hat_eavae.detach().cpu().numpy()[k])\n",
    "    break # just one batch\n",
    "\n",
    "x_nat_ori = []\n",
    "x_nat_vae = []\n",
    "x_nat_eavae = []\n",
    "for x, y in nat_test_rescaled_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x_hat_eavae, _, _, _, _ = model_eavae(x.to(model_eavae.device), only_mu=True, only_zmu=True)\n",
    "        x_hat_vae, _, _ = model_vae(x.to(model_vae.device), only_mu=True)\n",
    "    for i,k in enumerate(range(5,8)):\n",
    "        x_nat_ori.append(x.detach().cpu().numpy()[k])\n",
    "        x_nat_vae.append(x_hat_vae.detach().cpu().numpy()[k])\n",
    "        x_nat_eavae.append(x_hat_eavae.detach().cpu().numpy()[k])\n",
    "    break # just one batch\n",
    "\n",
    "x_shuff_ori = []\n",
    "x_shuff_vae = []\n",
    "x_shuff_eavae = []\n",
    "for x, y in num_test_ShuffPix_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x_hat_eavae, _, _, _, _ = model_eavae(x.to(model_eavae.device), only_mu=True, only_zmu=True)\n",
    "        x_hat_vae, _, _ = model_vae(x.to(model_vae.device), only_mu=True)\n",
    "    for i,k in enumerate(range(5,8)):\n",
    "        x_shuff_ori.append(x.detach().cpu().numpy()[k])\n",
    "        x_shuff_vae.append(x_hat_vae.detach().cpu().numpy()[k])\n",
    "        x_shuff_eavae.append(x_hat_eavae.detach().cpu().numpy()[k])\n",
    "    break # just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_location+'x_ori.pkl', 'wb') as f:\n",
    "    pickle.dump(x_ori, f)\n",
    "with open(out_location+'x_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_vae, f)\n",
    "with open(out_location+'x_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_eavae, f)\n",
    "    \n",
    "with open(out_location+'x_med_ori.pkl', 'wb') as f:\n",
    "    pickle.dump(x_med_ori, f)\n",
    "with open(out_location+'x_med_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_med_vae, f)\n",
    "with open(out_location+'x_med_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_med_eavae, f)\n",
    "    \n",
    "with open(out_location+'x_fash_ori.pkl', 'wb') as f:\n",
    "    pickle.dump(x_fash_ori, f)\n",
    "with open(out_location+'x_fash_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_fash_vae, f)\n",
    "with open(out_location+'x_fash_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_fash_eavae, f)\n",
    "    \n",
    "with open(out_location+'x_nat_ori.pkl', 'wb') as f:\n",
    "    pickle.dump(x_nat_ori, f)\n",
    "with open(out_location+'x_nat_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_nat_vae, f)\n",
    "with open(out_location+'x_nat_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_nat_eavae, f)\n",
    "    \n",
    "with open(out_location+'x_shuff_ori.pkl', 'wb') as f:\n",
    "    pickle.dump(x_shuff_ori, f)\n",
    "with open(out_location+'x_shuff_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_shuff_vae, f)\n",
    "with open(out_location+'x_shuff_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(x_shuff_eavae, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_df.to_pickle(out_location+'uncertainty_df')\n",
    "zmu_eavae_df.to_pickle(out_location+'zmu_eavae_df')\n",
    "uncertainty_vae_eavae_df.to_pickle(out_location+'uncertainty_vae_eavae_df')\n",
    "np.save(out_location+'num_uncertain_var_vae.npy',num_uncertain_var_vae)\n",
    "np.save(out_location+'num_uncertain_var_eavae.npy',num_uncertain_var_eavae)\n",
    "np.save(out_location+'num_avimg.npy',num_avimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes_per_category = [np.where(num_test_dataset.targets==j)[0] for j in range(10)]\n",
    "\n",
    "test_category_zmu_eavae = [num_test_zmu_eavae[test_indexes_per_category[j]].mean() for j in range(10)]\n",
    "test_category_mu_eavae = [num_test_mu_eavae[test_indexes_per_category[j]].mean(axis=0) for j in range(10)]\n",
    "test_category_cov_eavae = [np.diag(num_test_var_eavae[test_indexes_per_category[j]].mean(axis=0))+np.cov(num_test_mu_eavae[test_indexes_per_category[j]].T) for j in range(10)]\n",
    "\n",
    "test_category_mu_vae = [num_test_mu_vae[test_indexes_per_category[j]].mean(axis=0) for j in range(10)]\n",
    "test_category_cov_vae = [np.diag(num_test_var_vae[test_indexes_per_category[j]].mean(axis=0))+np.cov(num_test_mu_vae[test_indexes_per_category[j]].T) for j in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save test_category_mu_vae,test_category_cov_vae,test_indexes_per_category,num_test_mu_vae,num_test_var_vae to pickle\n",
    "with open(out_location+'test_category_mu_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(test_category_mu_vae, f)\n",
    "with open(out_location+'test_category_cov_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(test_category_cov_vae, f)\n",
    "with open(out_location+'test_indexes_per_category.pkl', 'wb') as f:\n",
    "    pickle.dump(test_indexes_per_category, f)\n",
    "with open(out_location+'num_test_mu_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(num_test_mu_vae, f)\n",
    "with open(out_location+'num_test_var_vae.pkl', 'wb') as f:\n",
    "    pickle.dump(num_test_var_vae, f)\n",
    "#same with eavae\n",
    "with open(out_location+'test_category_mu_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(test_category_mu_eavae, f)\n",
    "with open(out_location+'test_category_cov_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(test_category_cov_eavae, f)\n",
    "with open(out_location+'num_test_mu_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(num_test_mu_eavae, f)\n",
    "with open(out_location+'num_test_var_eavae.pkl', 'wb') as f:\n",
    "    pickle.dump(num_test_var_eavae, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eavae.eval()\n",
    "model_vae.eval()\n",
    "\n",
    "category_recons_vae,category_recons_eavae=[],[]\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(10):\n",
    "        category_recons_eavae.append(model_eavae.decoder(torch.tensor(test_category_mu_eavae[i]).unsqueeze(0).float().to(model_eavae.device),torch.tensor(test_category_zmu_eavae[i]).unsqueeze(0).float().to(model_eavae.device)).detach().cpu().numpy())        \n",
    "    for category_mu in test_category_mu_vae:\n",
    "        category_recons_vae.append(model_vae.decoder(torch.tensor(category_mu).unsqueeze(0).float().to(model_vae.device)).detach().cpu().numpy())\n",
    "        \n",
    "vae_fit = np.zeros((10,10,3))\n",
    "eavae_fit = np.zeros((10,10,3))\n",
    "\n",
    "int_cat_unc_vaes = []\n",
    "int_cat_unc_eavaes = []\n",
    "int_cat_zmu_eavaes = []\n",
    "interpolation_img_in_vaes = []\n",
    "interpolation_img_in_eavaes = []\n",
    "\n",
    "l=0\n",
    "for i in range(10):\n",
    "    for j in range(i+1,10):\n",
    "        interpolation_img_in_eavae, interpolation_img_out_eavae, interpolation_var_eavae, interpolation_mu_eavae, interpolation_zmu_eavae, interpolation_zvar_eavae = eavae_categoric_interpolation_line_pixel(model_eavae, category_recons_eavae, i, j, 50)\n",
    "        interpolation_img_in_vae,interpolation_img_out_vae , interpolation_var_vae, interpolation_mu_vae = vae_categoric_interpolation_line_pixel(model_vae, category_recons_vae, i, j, 50)\n",
    "        \n",
    "        lam_int = np.linspace(0, 1, 51)\n",
    "        int_cat_unc_eavae = np.sqrt(interpolation_var_eavae).mean(axis=1)\n",
    "        int_cat_zmu_eavae = np.exp(interpolation_zmu_eavae[:]+interpolation_zvar_eavae[:]/2)\n",
    "        eavae_fit[i,j] = np.polyfit(lam_int, int_cat_unc_eavae, 2)\n",
    "        int_cat_unc_vae = np.sqrt(interpolation_var_vae).mean(axis=1)\n",
    "        vae_fit[i,j] = np.polyfit(lam_int, int_cat_unc_vae, 2)\n",
    "\n",
    "        int_cat_unc_vaes.append(int_cat_unc_vae)\n",
    "        int_cat_unc_eavaes.append(int_cat_unc_eavae)\n",
    "        int_cat_zmu_eavaes.append(int_cat_zmu_eavae)\n",
    "        interpolation_img_in_vaes.append(interpolation_img_in_vae)\n",
    "        interpolation_img_in_eavaes.append(interpolation_img_in_eavae)\n",
    "        \n",
    "        l+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19497/1337229560.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  vae_in.append(((.5-thr<((-vae_fit[:,:,1]/(2*vae_fit[:,:,0])).flatten())) * (((-vae_fit[:,:,1]/(2*vae_fit[:,:,0])).flatten())<.5+thr) * ((vae_fit[:,:,0]).flatten()<0)).sum()/45)\n",
      "/tmp/ipykernel_19497/1337229560.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  eavae_in.append(((.5-thr<((-eavae_fit[:,:,1]/(2*eavae_fit[:,:,0])).flatten())) * (((-eavae_fit[:,:,1]/(2*eavae_fit[:,:,0])).flatten())<.5+thr) * ((eavae_fit[:,:,0]).flatten()<0)).sum()/45)\n"
     ]
    }
   ],
   "source": [
    "vae_in = []\n",
    "eavae_in = []\n",
    "for thr in np.linspace(0,.5,51):\n",
    "    vae_in.append(((.5-thr<((-vae_fit[:,:,1]/(2*vae_fit[:,:,0])).flatten())) * (((-vae_fit[:,:,1]/(2*vae_fit[:,:,0])).flatten())<.5+thr) * ((vae_fit[:,:,0]).flatten()<0)).sum()/45)\n",
    "    eavae_in.append(((.5-thr<((-eavae_fit[:,:,1]/(2*eavae_fit[:,:,0])).flatten())) * (((-eavae_fit[:,:,1]/(2*eavae_fit[:,:,0])).flatten())<.5+thr) * ((eavae_fit[:,:,0]).flatten()<0)).sum()/45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(out_location+'int_cat_unc_vaes.npy', np.array(int_cat_unc_vaes, dtype=float), allow_pickle=True)\n",
    "np.save(out_location+'int_cat_unc_eavaes.npy', np.array(int_cat_unc_eavaes, dtype=float), allow_pickle=True)\n",
    "np.save(out_location+'int_cat_zmu_eavaes.npy', np.array(int_cat_zmu_eavaes, dtype=float), allow_pickle=True)\n",
    "\n",
    "np.save(out_location+'interpolation_img_in_vaes.npy', np.array(interpolation_img_in_vaes, dtype=float), allow_pickle=True)\n",
    "np.save(out_location+'interpolation_img_in_eavaes.npy', np.array(interpolation_img_in_eavaes, dtype=float), allow_pickle=True)\n",
    "\n",
    "np.save(out_location+'vae_fit.npy',vae_fit)\n",
    "np.save(out_location+'eavae_fit.npy',eavae_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_mu_vae, num_train_var_vae, num_train_zmu_vae, num_train_zvar_vae = data_posteriors(model_vae, num_train_dataloader, latent_dim_vae, False)\n",
    "num_train_mu_eavae, num_train_var_eavae, num_train_zmu_eavae, num_train_zvar_eavae = data_posteriors(model_eavae, num_train_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "num_val_mu_vae, num_val_var_vae, num_val_zmu_vae, num_val_zvar_vae = data_posteriors(model_vae, num_validation_dataloader, latent_dim_vae, False)\n",
    "num_val_mu_eavae, num_val_var_eavae, num_val_zmu_eavae, num_val_zvar_eavae = data_posteriors(model_eavae, num_validation_dataloader, latent_dim_eavae, True)\n",
    "\n",
    "num_test_mu_vae, num_test_var_vae, num_test_zmu_vae, num_test_zvar_vae = data_posteriors(model_vae, num_test_dataloader, latent_dim_vae, False)\n",
    "num_test_mu_eavae, num_test_var_eavae, num_test_zmu_eavae, num_test_zvar_eavae = data_posteriors(model_eavae, num_test_dataloader, latent_dim_eavae, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train_vae = np.hstack((num_train_mu_vae,num_train_var_vae))\n",
    "W_val_vae = np.hstack((num_val_mu_vae,num_val_var_vae))\n",
    "W_test_vae = np.hstack((num_test_mu_vae,num_test_var_vae))\n",
    "\n",
    "y_train_vae = np.array(dataset.targets[num_train_dataset.indices].clone().detach())\n",
    "y_val_vae = np.array(dataset.targets[num_val_dataset.indices].clone().detach())\n",
    "y_test_vae = np.array(num_test_dataset.targets.clone().detach())\n",
    "\n",
    "\n",
    "W_train_eavae = np.hstack((num_train_mu_eavae,num_train_var_eavae))\n",
    "W_val_eavae = np.hstack((num_val_mu_eavae,num_val_var_eavae))\n",
    "W_test_eavae = np.hstack((num_test_mu_eavae,num_test_var_eavae))\n",
    "\n",
    "y_train_eavae = np.array(dataset.targets[num_train_dataset.indices].clone().detach())\n",
    "y_val_eavae = np.array(dataset.targets[num_val_dataset.indices].clone().detach())\n",
    "y_test_eavae = np.array(num_test_dataset.targets.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNS FOR ABOUT 1.5 HOURS, RESULTS CAN BE DIRECTLY IMPORTED BELOW\n",
    "\n",
    "clf_ws_vae_ = MLPClassifier(hidden_layer_sizes=(50, 50))  # Initialize the classifier\n",
    "score_train_vae_ = []\n",
    "score_val_vae_ = []\n",
    "best_val_score_vae = 0  # Start with the lowest possible score\n",
    "best_clf_vae = None  # Placeholder for the best classifier\n",
    "\n",
    "for k in range(1000):\n",
    "    # Generate synthetic training and validation data\n",
    "    X_s_train_vae = norm.rvs(size=(len(W_train_vae), latent_dim_vae)) * np.sqrt(W_train_vae[:, latent_dim_vae:]) + W_train_vae[:, 0:latent_dim_vae]\n",
    "    X_s_val_vae = norm.rvs(size=(len(W_val_vae), latent_dim_vae)) * np.sqrt(W_val_vae[:, latent_dim_vae:]) + W_val_vae[:, 0:latent_dim_vae]\n",
    "    # Update the model\n",
    "    clf_ws_vae_.partial_fit(X_s_train_vae, y_train_vae, classes=np.arange(10))\n",
    "    # Evaluate scores\n",
    "    train_score = clf_ws_vae_.score(X_s_train_vae, y_train_vae)\n",
    "    val_score = clf_ws_vae_.score(X_s_val_vae, y_val_vae)\n",
    "    # Save scores\n",
    "    score_train_vae_.append(train_score)\n",
    "    score_val_vae_.append(val_score)\n",
    "    # Check if current model is the best so far\n",
    "    if val_score > best_val_score_vae:\n",
    "        best_val_score_vae = val_score\n",
    "        best_clf_vae = clf_ws_vae_  # Save the current model\n",
    "    \n",
    "    \n",
    "clf_ws_eavae_ = MLPClassifier(hidden_layer_sizes=(50, 50))  # Initialize the classifier\n",
    "score_train_eavae_ = []\n",
    "score_val_eavae_ = []\n",
    "best_val_score_eavae = 0  # Start with the lowest possible score\n",
    "best_clf_eavae = None  # Placeholder for the best classifier\n",
    "\n",
    "for k in range(1000):\n",
    "    # Generate synthetic training and validation data\n",
    "    X_s_train_eavae = norm.rvs(size=(len(W_train_eavae), latent_dim_eavae)) * np.sqrt(W_train_eavae[:, latent_dim_eavae:]) + W_train_eavae[:, 0:latent_dim_eavae]\n",
    "    X_s_val_eavae = norm.rvs(size=(len(W_val_eavae), latent_dim_eavae)) * np.sqrt(W_val_eavae[:, latent_dim_eavae:]) + W_val_eavae[:, 0:latent_dim_eavae]\n",
    "    # Update the model\n",
    "    clf_ws_eavae_.partial_fit(X_s_train_eavae, y_train_eavae, classes=np.arange(10))\n",
    "    # Evaluate scores\n",
    "    train_score = clf_ws_eavae_.score(X_s_train_eavae, y_train_eavae)\n",
    "    val_score = clf_ws_eavae_.score(X_s_val_eavae, y_val_eavae)\n",
    "    # Save scores\n",
    "    score_train_eavae_.append(train_score)\n",
    "    score_val_eavae_.append(val_score)\n",
    "    # Check if current model is the best so far\n",
    "    if val_score > best_val_score_eavae:\n",
    "        best_val_score_eavae = val_score\n",
    "        best_clf_eavae = clf_ws_eavae_  # Save the current model\n",
    "\n",
    "#save\n",
    "with open(out_location+'clf_ws_vae.pkl','wb') as f:\n",
    "    pickle.dump(best_clf_vae,f)\n",
    "with open(out_location+'clf_ws_eavae.pkl','wb') as f:\n",
    "    pickle.dump(best_clf_eavae,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING MANUSCRIPT DATA, LOAD CLASSIFIERS\n",
    "with open(out_location+'clf_ws_vae.pkl', 'rb') as f:\n",
    "    clf_ws_vae = pkl.load(f)\n",
    "with open(out_location+'clf_ws_eavae.pkl', 'rb') as f:\n",
    "    clf_ws_eavae = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_mat_vae = np.zeros((len(med_test_mu_vae),10))\n",
    "med_mat_eavae = np.zeros((len(med_test_mu_eavae),10))\n",
    "\n",
    "Nsamp=1000\n",
    "for j in range(Nsamp):\n",
    "    np.random.seed(j)\n",
    "    med_mat_vae+=clf_ws_vae.predict_proba(norm.rvs(size=(len(med_test_mu_vae),latent_dim_vae))*np.sqrt(med_test_var_vae)+med_test_mu_vae)/Nsamp\n",
    "    np.random.seed(j)\n",
    "    med_mat_eavae+=clf_ws_eavae.predict_proba(norm.rvs(size=(len(med_test_mu_eavae),latent_dim_eavae))*np.sqrt(med_test_var_eavae)+med_test_mu_eavae)/Nsamp\n",
    "shuff_mat_vae = np.zeros((len(num_test_ShuffPix_mu_vae),10))\n",
    "shuff_mat_eavae = np.zeros((len(num_test_ShuffPix_mu_eavae),10))\n",
    "\n",
    "Nsamp=1000\n",
    "for j in range(Nsamp):\n",
    "    np.random.seed(j)\n",
    "    shuff_mat_vae+=clf_ws_vae.predict_proba(norm.rvs(size=(len(num_test_ShuffPix_mu_vae),latent_dim_vae))*np.sqrt(num_test_ShuffPix_var_vae)+num_test_ShuffPix_mu_vae)/Nsamp\n",
    "    np.random.seed(j)\n",
    "    shuff_mat_eavae+=clf_ws_eavae.predict_proba(norm.rvs(size=(len(num_test_ShuffPix_mu_eavae),latent_dim_eavae))*np.sqrt(num_test_ShuffPix_var_eavae)+num_test_ShuffPix_mu_eavae)/Nsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(out_location+'med_mat_vae.npy',med_mat_vae)\n",
    "np.save(out_location+'med_mat_eavae.npy',med_mat_eavae)\n",
    "np.save(out_location+'shuff_mat_vae.npy',shuff_mat_vae)\n",
    "np.save(out_location+'shuff_mat_eavae.npy',shuff_mat_eavae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_all_vae = []\n",
    "entropies_all_eavae = []\n",
    "\n",
    "for l in range(10):\n",
    "    for i in range(10):\n",
    "        if i>l:\n",
    "            predicts_vae = []\n",
    "            predicts_eavae = []\n",
    "            entropies_vae = []\n",
    "            entropies_eavae = []\n",
    "            for k in np.linspace(0,1,11):\n",
    "                model_vae.eval()\n",
    "                ins_vae, outs_vae, mus_vae, vars_vae = vae_fusion_numbers_pixel(model_vae, num_val_dataset, cat1=l, cat2=i, N=50, lam = k)\n",
    "                matt_vae = np.zeros((len(mus_vae),10))\n",
    "                for j in range(1000):\n",
    "                    X_s_vae = norm.rvs(size=(len(mus_vae),latent_dim_vae))*np.sqrt(vars_vae)+mus_vae\n",
    "                    matt_vae+=clf_ws_vae.predict_proba(X_s_vae)/1000\n",
    "                \n",
    "                predicts_vae.append(matt_vae.mean(axis=0))\n",
    "                entropies_vae.append(scipy.stats.entropy(matt_vae,axis=1).mean())\n",
    "                \n",
    "                model_eavae.eval()\n",
    "                ins_eavae, outs_eavae, mus_eavae, vars_eavae, zmus_eavae, zvars_eavae = eavae_fusion_numbers_pixel(model_eavae, num_val_dataset, cat1=l, cat2=i, N=50, lam = k)\n",
    "                matt_eavae = np.zeros((len(mus_eavae),10))\n",
    "                for j in range(1000):        \n",
    "                    X_s_eavae = norm.rvs(size=(len(mus_eavae),latent_dim_eavae))*np.sqrt(vars_eavae)+mus_eavae\n",
    "                    matt_eavae+=clf_ws_eavae.predict_proba(X_s_eavae)/1000\n",
    "                predicts_eavae.append(matt_eavae.mean(axis=0))\n",
    "                \n",
    "                entropies_eavae.append(scipy.stats.entropy(matt_eavae,axis=1).mean())\n",
    "            entropies_all_vae.append(np.array(entropies_vae))\n",
    "            entropies_all_eavae.append(np.array(entropies_eavae))\n",
    "            if l==2 and i==5:\n",
    "                np.save(out_location+f'predicts_vae_{l}_{i}.npy', np.array(predicts_vae), allow_pickle=True)\n",
    "                np.save(out_location+f'predicts_eavae_{l}_{i}.npy', np.array(predicts_eavae), allow_pickle=True)\n",
    "\n",
    "np.save(out_location+f'entropies_all_vae.npy', np.array(entropies_all_vae), allow_pickle=True)\n",
    "np.save(out_location+f'entropies_all_eavae.npy', np.array(entropies_all_eavae), allow_pickle=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
